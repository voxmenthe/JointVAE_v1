{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # change to your device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third_opinion_v01_data_dictionary.txt  third_opinion_v01.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/vmldata/raw_source_data/v20181105_third_opinion_v01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/vmldata/raw_source_data/v20181105_third_opinion_v01/third_opinion_v01.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/vmldata/raw_source_data/v20181105_third_opinion_v01/third_opinion_v01.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create list of image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = '/home/jovyan/vmldata/raw_source_data/v20181105_third_opinion_v01/third_opinion_v01.h5'\n",
    "third = pd.read_hdf(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>DSL_label</th>\n",
       "      <th>DL_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/1/5/1533245_2188111.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/2/4/2403009_2991265.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/2/8/2814416_3549033.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/2/6/2671073_3328473.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/2/8/2825395_3667369.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       path  DSL_label  DL_label\n",
       "0  /1/5/1533245_2188111.jpg          2         1\n",
       "1  /2/4/2403009_2991265.jpg          2         1\n",
       "2  /2/8/2814416_3549033.jpg          0         3\n",
       "3  /2/6/2671073_3328473.jpg          3         0\n",
       "4  /2/8/2825395_3667369.jpg          2         1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third_opinion_v01.h5 has three columns:\n",
      "\n",
      "path:       path to the file, should be attempted from the largest dataset as of Nov 5, 2018.\n",
      "ie: /vmldata/raw_source_data/v20180810_all_wearables\n",
      "\n",
      "DSL_label:  Dress Sleeve Length\n",
      "Value:  Meaning:       Definition:\n",
      "0:      No sleeve    - armless dress with nothing sticking out that appears to curve down towards shoulder\n",
      "1:      short sleeve - sleeveless with shoulder that curves down towards arm to elbow length\n",
      "2:      3/4 sleeve   - elbow length to approximately wrist length\n",
      "3:      long         - from wrist length to anything longer\n",
      "\n",
      "DL_label:   Dress Length\n",
      "Value:  Meaning:       Definition:\n",
      "0:      Short        - Shorter than clearly above the knee area\n",
      "1:      KneeLength   - Hem of dress is at knee height (all of knee cap/tendon area)\n",
      "2:      Midi         - Below knee lower tendon to approximately top of ankle area\n",
      "3:      Long         - Anything touching the ankles or longer\n",
      "4:      HiLo         - Normally any dress (regardless of length) that has a higher front hem than back\n",
      "hem (as an OBVIOUS style feature)  Somewhat off use case dresses that are sideways\n",
      "HiLo also get this designation.  Imagine diagonal slash with left being high hem\n",
      "and  right being low hemline, or right high, left low.\n"
     ]
    }
   ],
   "source": [
    "labs = '/home/jovyan/vmldata/raw_source_data/v20181105_third_opinion_v01/third_opinion_v01_data_dictionary.txt'\n",
    "with open(labs) as labels_dict:\n",
    "    for line in labels_dict:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "longish = third.query('DL_label > 0 and DL_label < 4')\n",
    "ls = longish.query('DSL_label > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10775 6350 3961\n"
     ]
    }
   ],
   "source": [
    "print(len(third), len(longish), len(longandsleeve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/1/5/1533245_2188111.jpg',\n",
       " '/2/4/2403009_2991265.jpg',\n",
       " '/2/8/2825395_3667369.jpg',\n",
       " '/2/9/2979794_3871671.jpg',\n",
       " '/2/5/2596552_3418185.jpg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ls['path'][:5])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bad_data = ['/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/7/2/723739_1342692.jpg']\n",
    "\n",
    "for bad in bad_data:\n",
    "    if bad in loadable_dresses:\n",
    "        del loadable_dresses[loadable_dresses.index(bad)]\n",
    "        \n",
    "print(len(loadable_dresses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train image paths: 3,400\n",
      "Number of test image paths: 560\n",
      "\n",
      "Sample paths:\n",
      "/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/1/5/1533245_2188111.jpg\n",
      "/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/2/2/2225108_2817244.jpg\n",
      "/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/4/3/4384848_8196944.jpg\n",
      "/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/2/3/2328189_2681756.jpg\n"
     ]
    }
   ],
   "source": [
    "#image_paths_train = loadable_dresses[:8000]\n",
    "#image_paths_test = loadable_dresses[9000:11000]\n",
    "\n",
    "rootpath = '/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables'\n",
    "lsdata = [rootpath + x for x in ls['path']]\n",
    "\n",
    "image_paths_train = lsdata[:3400]\n",
    "image_paths_test = lsdata[3400:-1]\n",
    "\n",
    "print(f\"Number of train image paths: {len(image_paths_train):,d}\")\n",
    "print(f\"Number of test image paths: {len(image_paths_test):,d}\")\n",
    "print()\n",
    "print(\"Sample paths:\")\n",
    "print(image_paths_train[0])\n",
    "print(image_paths_train[-1])\n",
    "print(image_paths_test[0])\n",
    "print(image_paths_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "#from utils.dataloaders_custom import get_imagelist_dataloader, ImageListDataset\n",
    "from utils.dataloader_tools import get_imagelist_dataloader, ImageListDataset\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "composed = transforms.Compose([transforms.CenterCrop((256,256)),transforms.Resize((256,256)),transforms.ToTensor()])\n",
    "\n",
    "# convert rgb is for the cv2 loaded images that I've got in this dir\n",
    "conv_rgb = False\n",
    "error_handling = True\n",
    "train_dataset = ImageListDataset(image_paths_train, cut_from='top', cut_amount=256, \n",
    "                                 transform=composed, convert_rgb=conv_rgb, error_handling=True)\n",
    "test_dataset = ImageListDataset(image_paths_test, cut_from='top', cut_amount=256, \n",
    "                                 transform=composed, convert_rgb=conv_rgb, error_handling=True)\n",
    "\n",
    "train_loader = get_imagelist_dataloader(batch_size=BATCH_SIZE, dataset_object=train_dataset)\n",
    "test_loader = get_imagelist_dataloader(batch_size=BATCH_SIZE, dataset_object=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 16,\n",
    "               'disc': [16]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointvae.models_256_xstyle_int_nd import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, img_size=(3, 256, 256), use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, amsgrad=True) # added amsgrad # orig lr 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointvae.training import Trainer\n",
    "#from jointvae.training_debug import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 4.8, 30000, 32.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 4.8, 30000, 32.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,\n",
    "                 use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from viz.visualize import Visualizer\n",
    "from viz.visualize import Visualizer\n",
    "\n",
    "viz = Visualizer(model)\n",
    "viz.save_images = False # needed to add this so it returns a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3400\tLoss: 45078.770\n",
      "1000/3400\tLoss: 44949.850\n",
      "2000/3400\tLoss: 43167.425\n",
      "3000/3400\tLoss: 34748.878\n",
      "Epoch: 1 Average loss: 39872.81\n",
      "0/3400\tLoss: 30530.840\n",
      "1000/3400\tLoss: 27578.596\n",
      "2000/3400\tLoss: 26045.109\n",
      "3000/3400\tLoss: 25566.692\n",
      "Epoch: 2 Average loss: 26290.99\n",
      "0/3400\tLoss: 25495.051\n",
      "1000/3400\tLoss: 24818.844\n",
      "2000/3400\tLoss: 24548.526\n",
      "3000/3400\tLoss: 23646.862\n",
      "Epoch: 3 Average loss: 24123.26\n",
      "0/3400\tLoss: 20583.135\n",
      "1000/3400\tLoss: 19733.020\n",
      "2000/3400\tLoss: 17411.778\n",
      "3000/3400\tLoss: 16614.237\n",
      "Epoch: 4 Average loss: 17755.19\n",
      "0/3400\tLoss: 15842.629\n",
      "1000/3400\tLoss: 16166.857\n",
      "2000/3400\tLoss: 15989.766\n",
      "3000/3400\tLoss: 16000.189\n",
      "Epoch: 5 Average loss: 16015.24\n",
      "0/3400\tLoss: 16222.580\n",
      "1000/3400\tLoss: 15646.666\n",
      "2000/3400\tLoss: 15733.023\n",
      "3000/3400\tLoss: 15557.559\n",
      "Epoch: 6 Average loss: 15669.42\n",
      "0/3400\tLoss: 14751.002\n",
      "1000/3400\tLoss: 15497.164\n",
      "2000/3400\tLoss: 15660.093\n",
      "3000/3400\tLoss: 15365.657\n",
      "Epoch: 7 Average loss: 15504.82\n",
      "0/3400\tLoss: 15549.275\n",
      "1000/3400\tLoss: 15503.600\n",
      "2000/3400\tLoss: 15211.424\n",
      "3000/3400\tLoss: 15301.693\n",
      "Epoch: 8 Average loss: 15337.79\n",
      "0/3400\tLoss: 16148.247\n",
      "1000/3400\tLoss: 15372.951\n",
      "2000/3400\tLoss: 15257.779\n",
      "3000/3400\tLoss: 15102.974\n",
      "Epoch: 9 Average loss: 15219.79\n",
      "0/3400\tLoss: 15616.983\n",
      "1000/3400\tLoss: 15194.494\n",
      "2000/3400\tLoss: 15140.858\n",
      "3000/3400\tLoss: 14964.454\n",
      "Epoch: 10 Average loss: 15099.39\n",
      "0/3400\tLoss: 15642.713\n",
      "1000/3400\tLoss: 15228.705\n",
      "2000/3400\tLoss: 14873.755\n",
      "3000/3400\tLoss: 14931.250\n",
      "Epoch: 11 Average loss: 15026.78\n",
      "0/3400\tLoss: 13331.032\n",
      "1000/3400\tLoss: 14952.060\n",
      "2000/3400\tLoss: 15021.429\n",
      "3000/3400\tLoss: 14971.657\n",
      "Epoch: 12 Average loss: 14974.22\n",
      "0/3400\tLoss: 15178.769\n",
      "1000/3400\tLoss: 14815.467\n",
      "2000/3400\tLoss: 14948.777\n",
      "3000/3400\tLoss: 14875.143\n",
      "Epoch: 13 Average loss: 14915.25\n",
      "0/3400\tLoss: 14754.517\n",
      "1000/3400\tLoss: 14802.070\n",
      "2000/3400\tLoss: 14846.106\n",
      "3000/3400\tLoss: 15055.857\n",
      "Epoch: 14 Average loss: 14879.82\n",
      "0/3400\tLoss: 14862.133\n",
      "1000/3400\tLoss: 14790.087\n",
      "2000/3400\tLoss: 14749.183\n",
      "3000/3400\tLoss: 14822.957\n",
      "Epoch: 15 Average loss: 14803.53\n",
      "0/3400\tLoss: 15321.403\n",
      "1000/3400\tLoss: 14824.972\n",
      "2000/3400\tLoss: 14867.884\n",
      "3000/3400\tLoss: 14724.870\n",
      "Epoch: 16 Average loss: 14806.94\n",
      "0/3400\tLoss: 15373.495\n",
      "1000/3400\tLoss: 14726.408\n",
      "2000/3400\tLoss: 14856.050\n",
      "3000/3400\tLoss: 14692.461\n",
      "Epoch: 17 Average loss: 14751.75\n",
      "0/3400\tLoss: 14108.140\n",
      "1000/3400\tLoss: 14855.167\n",
      "2000/3400\tLoss: 14644.363\n",
      "3000/3400\tLoss: 14730.218\n",
      "Epoch: 18 Average loss: 14729.15\n",
      "0/3400\tLoss: 13490.379\n",
      "1000/3400\tLoss: 14871.432\n",
      "2000/3400\tLoss: 14607.950\n",
      "3000/3400\tLoss: 14671.776\n",
      "Epoch: 19 Average loss: 14712.22\n",
      "0/3400\tLoss: 14644.794\n",
      "1000/3400\tLoss: 14661.669\n",
      "2000/3400\tLoss: 14703.767\n",
      "3000/3400\tLoss: 14704.772\n",
      "Epoch: 20 Average loss: 14682.02\n",
      "0/3400\tLoss: 14611.810\n",
      "1000/3400\tLoss: 14538.754\n",
      "2000/3400\tLoss: 14815.301\n",
      "3000/3400\tLoss: 14570.813\n",
      "Epoch: 21 Average loss: 14656.73\n",
      "0/3400\tLoss: 14082.350\n",
      "1000/3400\tLoss: 14729.702\n",
      "2000/3400\tLoss: 14559.570\n",
      "3000/3400\tLoss: 14701.084\n",
      "Epoch: 22 Average loss: 14640.20\n",
      "0/3400\tLoss: 15255.946\n",
      "1000/3400\tLoss: 14586.507\n",
      "2000/3400\tLoss: 14606.681\n",
      "3000/3400\tLoss: 14606.095\n",
      "Epoch: 23 Average loss: 14612.37\n",
      "0/3400\tLoss: 14481.761\n",
      "1000/3400\tLoss: 14764.491\n",
      "2000/3400\tLoss: 14617.003\n",
      "3000/3400\tLoss: 14471.131\n",
      "Epoch: 24 Average loss: 14607.60\n",
      "0/3400\tLoss: 13955.180\n",
      "1000/3400\tLoss: 14528.323\n",
      "2000/3400\tLoss: 14633.825\n",
      "3000/3400\tLoss: 14551.180\n",
      "Epoch: 25 Average loss: 14582.21\n",
      "0/3400\tLoss: 13555.458\n",
      "1000/3400\tLoss: 14579.944\n",
      "2000/3400\tLoss: 14474.513\n",
      "3000/3400\tLoss: 14636.448\n",
      "Epoch: 26 Average loss: 14586.35\n",
      "0/3400\tLoss: 14685.774\n",
      "1000/3400\tLoss: 14438.203\n",
      "2000/3400\tLoss: 14508.947\n",
      "3000/3400\tLoss: 14730.633\n",
      "Epoch: 27 Average loss: 14562.43\n",
      "0/3400\tLoss: 15177.316\n",
      "1000/3400\tLoss: 14320.362\n",
      "2000/3400\tLoss: 14548.801\n",
      "3000/3400\tLoss: 14732.134\n",
      "Epoch: 28 Average loss: 14526.89\n",
      "0/3400\tLoss: 13887.581\n",
      "1000/3400\tLoss: 14427.562\n",
      "2000/3400\tLoss: 14569.273\n",
      "3000/3400\tLoss: 14536.347\n",
      "Epoch: 29 Average loss: 14522.74\n",
      "0/3400\tLoss: 14533.652\n",
      "1000/3400\tLoss: 14440.288\n",
      "2000/3400\tLoss: 14570.029\n",
      "3000/3400\tLoss: 14555.762\n",
      "Epoch: 30 Average loss: 14516.99\n",
      "0/3400\tLoss: 14099.146\n",
      "1000/3400\tLoss: 14498.137\n",
      "2000/3400\tLoss: 14523.617\n",
      "3000/3400\tLoss: 14491.862\n",
      "Epoch: 31 Average loss: 14495.65\n",
      "0/3400\tLoss: 14247.311\n",
      "1000/3400\tLoss: 14463.941\n",
      "2000/3400\tLoss: 14517.915\n",
      "3000/3400\tLoss: 14406.917\n",
      "Epoch: 32 Average loss: 14467.69\n",
      "0/3400\tLoss: 15972.998\n",
      "1000/3400\tLoss: 14543.864\n",
      "2000/3400\tLoss: 14435.686\n",
      "3000/3400\tLoss: 14491.114\n",
      "Epoch: 33 Average loss: 14476.06\n",
      "0/3400\tLoss: 13579.278\n",
      "1000/3400\tLoss: 14441.318\n",
      "2000/3400\tLoss: 14525.932\n",
      "3000/3400\tLoss: 14409.060\n",
      "Epoch: 34 Average loss: 14455.54\n",
      "0/3400\tLoss: 14835.417\n",
      "1000/3400\tLoss: 14501.807\n",
      "2000/3400\tLoss: 14551.505\n",
      "3000/3400\tLoss: 14329.829\n",
      "Epoch: 35 Average loss: 14447.53\n",
      "0/3400\tLoss: 14437.033\n",
      "1000/3400\tLoss: 14564.738\n",
      "2000/3400\tLoss: 14390.673\n",
      "3000/3400\tLoss: 14296.256\n",
      "Epoch: 36 Average loss: 14429.02\n",
      "0/3400\tLoss: 14867.602\n",
      "1000/3400\tLoss: 14387.446\n",
      "2000/3400\tLoss: 14615.744\n",
      "3000/3400\tLoss: 14244.687\n",
      "Epoch: 37 Average loss: 14428.33\n",
      "0/3400\tLoss: 14840.062\n",
      "1000/3400\tLoss: 14339.465\n",
      "2000/3400\tLoss: 14407.019\n",
      "3000/3400\tLoss: 14495.938\n",
      "Epoch: 38 Average loss: 14419.88\n",
      "0/3400\tLoss: 14742.037\n",
      "1000/3400\tLoss: 14358.743\n",
      "2000/3400\tLoss: 14381.820\n",
      "3000/3400\tLoss: 14420.716\n",
      "Epoch: 39 Average loss: 14407.56\n",
      "0/3400\tLoss: 14147.825\n",
      "1000/3400\tLoss: 14360.938\n",
      "2000/3400\tLoss: 14499.989\n",
      "3000/3400\tLoss: 14275.903\n",
      "Epoch: 40 Average loss: 14399.53\n",
      "0/3400\tLoss: 14660.737\n",
      "1000/3400\tLoss: 14504.721\n",
      "2000/3400\tLoss: 14436.213\n",
      "3000/3400\tLoss: 14395.686\n",
      "Epoch: 41 Average loss: 14415.08\n",
      "0/3400\tLoss: 12441.448\n",
      "1000/3400\tLoss: 14400.497\n",
      "2000/3400\tLoss: 14505.360\n",
      "3000/3400\tLoss: 14329.699\n",
      "Epoch: 42 Average loss: 14388.19\n",
      "0/3400\tLoss: 14532.424\n",
      "1000/3400\tLoss: 14386.239\n",
      "2000/3400\tLoss: 14357.776\n",
      "3000/3400\tLoss: 14370.068\n",
      "Epoch: 43 Average loss: 14377.99\n",
      "0/3400\tLoss: 14210.371\n",
      "1000/3400\tLoss: 14373.826\n",
      "2000/3400\tLoss: 14371.686\n",
      "3000/3400\tLoss: 14402.154\n",
      "Epoch: 44 Average loss: 14384.24\n",
      "0/3400\tLoss: 15515.312\n",
      "1000/3400\tLoss: 14417.347\n",
      "2000/3400\tLoss: 14252.375\n",
      "3000/3400\tLoss: 14383.914\n",
      "Epoch: 45 Average loss: 14363.20\n",
      "0/3400\tLoss: 14868.899\n"
     ]
    }
   ],
   "source": [
    "# Train model for 10 epochs\n",
    "# Note this should really be a 100 epochs and trained on a GPU, but this is just to demo\n",
    "\n",
    "trainer.train(train_loader, epochs=50, save_training_gif=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "print(\"batch: \",type(batch),batch.shape)\n",
    "type(viz.reconstructions(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(batch)\n",
    "\n",
    "plt.figure(figsize=(26,26))\n",
    "plt.imshow(np.rot90(np.transpose(recon.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_reconstructions.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "samples = viz.samples()\n",
    "\n",
    "plt.figure(figsize=(26,26))\n",
    "plt.imshow(np.rot90(np.transpose(samples.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_samples.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traverses all latent dimensions one by one and plots a grid of images where each row corresponds to a latent traversal of one latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "traversals = viz.all_latent_traversals(size=10)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_all_traversals_n20.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=2, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_traversals2100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=1, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(traversals.numpy()[0, :, :]);\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_traversals1100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=9, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(\"sample_images/home/256/xint256_e600_b100_c10d10_cap48_30k_gam32_traversals9100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xint256_e600_b100_c10d10_cap48_30k_gam32.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"trained_models\" + \"statedict_\" + model_name) # save state dict\n",
    "#torch.save(model, model_name) # save full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done training: \",model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Model from State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sd_model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64))\n",
    "sd_model.load_state_dict(torch.load(\"statedict_\" + model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Full Model\n",
    "* Note in this case the serialized data is bound to the specific classes and exact directory strucutre used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
