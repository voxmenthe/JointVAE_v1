{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a JointVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # change to your device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dress_dresslen_train_test_splits.json\tloadable_women_primary_dress.csv\r\n",
      "dress_sleeve_train_test_splits.json\trayimages.ipynb\r\n",
      "dress_sleevelen_train_test_splits.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dress sleeve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train_1', 'y_train_1', 'X_test_1', 'y_test_1', 'X_train_2', 'y_train_2', 'X_test_2', 'y_test_2', 'X_train_3', 'y_train_3', 'X_test_3', 'y_test_3', 'X_train_4', 'y_train_4', 'X_test_4', 'y_test_4', 'X_train_5', 'y_train_5', 'X_test_5', 'y_test_5', 'X_train_6', 'y_train_6', 'X_test_6', 'y_test_6', 'X_train_7', 'y_train_7', 'X_test_7', 'y_test_7', 'X_train_8', 'y_train_8', 'X_test_8', 'y_test_8', 'X_train_9', 'y_train_9', 'X_test_9', 'y_test_9', 'X_train_10', 'y_train_10', 'X_test_10', 'y_test_10'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./data/dress_sleeve_train_test_splits.json\", \"r\") as infile:\n",
    "    data_dict = json.load(infile)\n",
    "    \n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify paths for home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  2  3  4  5  6  7  8\t9\r\n"
     ]
    }
   ],
   "source": [
    "!ls /workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_train = []\n",
    "image_paths_test = []\n",
    "\n",
    "#root_data_dir = \"/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables\"\n",
    "root_data_dir = \"/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810\"\n",
    "\n",
    "for key, val in data_dict.items():\n",
    "    if 'X_train' in key:\n",
    "        image_paths_train.extend([root_data_dir + imgpath for imgpath in val])\n",
    "    elif 'X_test' in key:\n",
    "        image_paths_test.extend([root_data_dir + imgpath for imgpath in val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get ALL the filenames actually there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/5/5/5594593_9244920.jpg',\n",
       " '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/5/5/5508248_9177849.jpg',\n",
       " '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/5/5/5520209_8572840.jpg',\n",
       " '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/5/5/5524461_9299582.jpg',\n",
       " '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/5/5/5548707_8853300.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_filenames = []\n",
    "for filename in glob.iglob(root_data_dir + '**/*/*/*', recursive=True):\n",
    "     all_filenames.append(filename)\n",
    "\n",
    "all_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolve conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207603 23067\n"
     ]
    }
   ],
   "source": [
    "print(len(image_paths_train), len(image_paths_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230670\n"
     ]
    }
   ],
   "source": [
    "both = image_paths_train + image_paths_test\n",
    "print(len(both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_diff1=np.setdiff1d(both,all_filenames)\n",
    "#diffs = list(set(both) ^ set(all_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numpy_diff1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(numpy_diff1[0] in both)\n",
    "print(numpy_diff1[0] in all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list(full_list, excludes):\n",
    "    s = set(excludes)\n",
    "    return (x for x in full_list if x not in s)\n",
    "#filtered_list = list(filter_list(full_list, excludes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = list(filter_list(both, numpy_diff1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224080\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train image paths: 190,000\n",
      "Number of test image paths: 34,080\n",
      "\n",
      "Sample paths:\n",
      "/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/8/2893552_3773662.jpg\n",
      "/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/6/2694064_3474675.jpg\n",
      "/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/9/2980076_3864391.jpg\n",
      "/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/3/0/3058817_8191784.jpg\n"
     ]
    }
   ],
   "source": [
    "image_paths_train = cleaned[:190000]\n",
    "image_paths_test = cleaned[190000:]\n",
    "\n",
    "print(f\"Number of train image paths: {len(image_paths_train):,d}\")\n",
    "print(f\"Number of test image paths: {len(image_paths_test):,d}\")\n",
    "print()\n",
    "print(\"Sample paths:\")\n",
    "print(image_paths_train[0])\n",
    "print(image_paths_train[-1])\n",
    "print(image_paths_test[0])\n",
    "print(image_paths_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not sure this is actually bad data - maybe something wrong with path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/2/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imgs_not_found_home = [\n",
    "    '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/2/2255571_2844023.jpg',\n",
    "    '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/0/2093772_3663763.jpg',\n",
    "    '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/1/9/1916308_2664495.jpg',\n",
    "    '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/2/2255551_2843963.jpg',\n",
    "    '/workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/2/2/2247175_3085324.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loadable dresses data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loadable_dresses = list(np.loadtxt('data/loadable_women_primary_dress.csv',delimiter=',',skiprows=1,dtype='str'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(loadable_dresses))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loadable_dresses[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bad_data = ['/home/jovyan/vmldata/raw_source_data/v20180810_all_wearables/7/2/723739_1342692.jpg']\n",
    "\n",
    "for bad in bad_data:\n",
    "    if bad in loadable_dresses:\n",
    "        del loadable_dresses[loadable_dresses.index(bad)]\n",
    "        \n",
    "print(len(loadable_dresses))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "image_paths_train = loadable_dresses[:88800]\n",
    "image_paths_test = loadable_dresses[88800:-124]\n",
    "\n",
    "print(f\"Number of train image paths: {len(image_paths_train):,d}\")\n",
    "print(f\"Number of test image paths: {len(image_paths_test):,d}\")\n",
    "print()\n",
    "print(\"Sample paths:\")\n",
    "print(image_paths_train[0])\n",
    "print(image_paths_train[-1])\n",
    "print(image_paths_test[0])\n",
    "print(image_paths_test[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from utils.dataloader_tools import get_imagelist_dataloader, ImageListDataset\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "composed = transforms.Compose([transforms.CenterCrop((256,256)),transforms.Resize((256,256)),transforms.ToTensor()])\n",
    "\n",
    "# convert rgb is for the cv2 loaded images that I've got in this dir\n",
    "train_dataset = ImageListDataset(image_paths_train, cut_from='top', cut_amount=256, transform=composed, \n",
    "                                 convert_rgb=False, error_handling=True)\n",
    "test_dataset = ImageListDataset(image_paths_test, cut_from='top', cut_amount=256, transform=composed, \n",
    "                                convert_rgb=False, error_handling=True)\n",
    "\n",
    "train_loader = get_imagelist_dataloader(batch_size=BATCH_SIZE, dataset_object=train_dataset)\n",
    "test_loader = get_imagelist_dataloader(batch_size=BATCH_SIZE, dataset_object=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent distribution of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent distribution will be joint distribution of 10 gaussian normal distributions\n",
    "# and one 10 dimensional Gumbel Softmax distribution\n",
    "latent_spec = {'cont': 20, 'disc': [10, 10, 10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointvae.models_256_convjump2bn import VAE\n",
    "\n",
    "model = VAE(latent_spec=latent_spec, hidden_dim=1024, img_size=(3, 256, 256), use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Build optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=8e-4, amsgrad=True) # added amsgrad # orig lr 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointvae.training import Trainer\n",
    "#from jointvae.training_debug import Trainer\n",
    "\n",
    "# Define the capacities\n",
    "# Continuous channels\n",
    "cont_capacity = [0.0, 7, 40000, 36.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "# Discrete channels\n",
    "disc_capacity = [0.0, 7, 40000, 36.0]  # Starting at a capacity of 0.0, increase this to 5.0\n",
    "                                         # over 25000 iterations with a gamma of 30.0\n",
    "\n",
    "# Build a trainer\n",
    "trainer = Trainer(model, optimizer,\n",
    "                  cont_capacity=cont_capacity,\n",
    "                  disc_capacity=disc_capacity,\n",
    "                 use_cuda=use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from viz.visualize import Visualizer\n",
    "from viz.visualize import Visualizer\n",
    "\n",
    "viz = Visualizer(model)\n",
    "viz.save_images = False # needed to add this so it returns a tensor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#from viz.visualize import Visualizer\n",
    "from viz.visualize_extra_v1 import Visualizer as Visualizer2\n",
    "\n",
    "viz2 = Visualizer2(model)\n",
    "viz2.save_images = False # needed to add this so it returns a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/190000\tLoss: 45369.492\n",
      "25600/190000\tLoss: 36886.453\n"
     ]
    }
   ],
   "source": [
    "# Note this should be at least 100 epochs for proper training or more but can be less to demo\n",
    "\n",
    "trainer.train(train_loader, epochs=152, save_training_gif=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls /workspace/mnt/crucial_2TB/111_Extra_Data/Macys/all_wearables20180810/1/9/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image_path = \"sample_images/home/256\"\n",
    "model_name = \"cj2_e152_b512_c20d10-10-10_cmax7_gam36\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstructions\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Get a batch of data\n",
    "for batch, labels in test_loader:\n",
    "    break\n",
    "\n",
    "# Reconstruct data using Joint-VAE model\n",
    "recon = viz.reconstructions(batch)\n",
    "\n",
    "plt.figure(figsize=(26,26))\n",
    "plt.imshow(np.rot90(np.transpose(recon.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_reconstructions.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot samples\n",
    "samples = viz.samples()\n",
    "\n",
    "plt.figure(figsize=(26,26))\n",
    "plt.imshow(np.rot90(np.transpose(samples.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_samples.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Traversals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traverses all latent dimensions one by one and plots a grid of images where each row corresponds to a latent traversal of one latent dimension\n",
    "* size: Number of samples for each latent traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(viz.model.latent_cont_dim, viz.model.num_disc_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all traversals\n",
    "traversals = viz.all_latent_traversals(size=20)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_all_traversals_n20.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=2, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals2100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=1, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(traversals.numpy()[0, :, :]);\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals1100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=9, cont_axis=1, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals9100.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=2, cont_axis=1, disc_idx=0, disc_axis=1, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals2101.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=2, cont_axis=0, disc_idx=0, disc_axis=1, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals2001.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=3, cont_axis=0, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals3000.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz.latent_traversal_grid(cont_idx=12, cont_axis=0, disc_idx=0, disc_axis=0, size=(10, 10))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "plt.savefig(save_image_path + model_name + \"_traversals3000.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a grid of some traversals\n",
    "traversals = viz2.latent_traversal_grid2(cont_idx=3, cont_axis=0, disc_idx=0, disc_axis=0, size=(20, 20),first_n=10)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.rot90(np.transpose(traversals.numpy(),(2,1,0)),k=3));\n",
    "#plt.savefig(\"sample_images/306/256/cj1_256_e50maybe_b64_c10d10-10-10_gam30_traversals3000.png\",dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"trained_models/\" + \"statedict_\" + model_name) # save state dict\n",
    "#torch.save(model, model_name) # save full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done training: \",model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Model from State Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sd_model = VAE(latent_spec=latent_spec, img_size=(3, 64, 64))\n",
    "sd_model.load_state_dict(torch.load(\"statedict_\" + model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Full Model\n",
    "* Note in this case the serialized data is bound to the specific classes and exact directory strucutre used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = torch.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
